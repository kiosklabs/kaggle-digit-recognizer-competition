{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first attempt, we will be using random forest classifier to fit our classifer to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "training_data = pd.read_csv(\"data/digit-train.csv\")\n",
    "test_data = pd.read_csv(\"data/digit-test.csv\").values\n",
    "\n",
    "target = training_data[[0]].values.ravel()\n",
    "train = training_data.iloc[:,1:].values\n",
    "test = pd.read_csv(\"data/digit-test.csv\").values\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(train, target)\n",
    "pred = rf.predict(test)\n",
    "\n",
    "np.savetxt('data/kaggle-digit-classifier-2016-07-27.csv', np.c_[range(1,len(test)+1),pred], delimiter=',', header = 'ImageId,Label', comments = '', fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After submitting this in kaggle, the accuracy gives us around 0.96 which is the entry level. This is similar to the accuracy of the example in the scikit-learn documentation using Support Vector Classification (http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html)\n",
    "\n",
    "However, as we can see in the MNIST page (http://yann.lecun.com/exdb/mnist/), convolutional neural networks would yield the most accurate prediction with a test error rate that ranges from 1.7 to 0.23, compared to neural nets (4.7-0.35) and SVMs (1.4 -0.56)\n",
    "\n",
    "Let's identify first the data, then plot it to get a glimpse of the actual image, so that we can prepare the needed layers for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "print(training_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = target.astype(np.uint8)\n",
    "train = np.array(train).reshape((-1, 1, 28, 28)).astype(np.uint8)\n",
    "test = np.array(test).reshape((-1, 1, 28, 28)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to re-evaluate (reshape) and transform our training data which has 784 columns of pixel data into a 28 x 28 grids. We'll be using matplotlib to graph this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fef946fa590>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.imshow(train[1729][0], cmap=cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using nolearn to build our convolutional neural network. Nolearn is based on the lasagne library, which allows us to build neural networks.\n",
    "\n",
    "Lasagne and Nolearn can be installed by using pip:\n",
    "- pip install -r https://raw.githubusercontent.com/Lasagne/Lasagne/master/requirements.txt\n",
    "\n",
    "- pip install -r https://raw.githubusercontent.com/dnouri/nolearn/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rogue/venv/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "from lasagne import layers\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from nolearn.lasagne import visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a new neural network that will train the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_net = NeuralNet(\n",
    "    layers=[('input', layers.InputLayer),\n",
    "            ('hidden', layers.DenseLayer),\n",
    "            ('output', layers.DenseLayer),\n",
    "    ],\n",
    "\n",
    "    input_shape=(None,1,28,28), #input layer\n",
    "    hidden_num_units=1000, #hidden layer\n",
    "    output_nonlinearity=lasagne.nonlinearities.softmax, #softmax\n",
    "    output_num_units=10, #target values\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.0001,\n",
    "    update_momentum=0.9,\n",
    "    max_epochs=15,\n",
    "    verbose=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 795010 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name    size\n",
      "---  ------  -------\n",
      "  0  input   1x28x28\n",
      "  1  hidden  1000\n",
      "  2  output  10\n",
      "\n",
      "  epoch    trn loss    val loss    trn/val    valid acc  dur\n",
      "-------  ----------  ----------  ---------  -----------  -----\n",
      "      1     \u001b[36m8.00096\u001b[0m     \u001b[32m1.78633\u001b[0m    4.47899      0.92945  4.27s\n",
      "      2     \u001b[36m0.89166\u001b[0m     \u001b[32m1.27761\u001b[0m    0.69792      0.93896  4.01s\n",
      "      3     \u001b[36m0.39200\u001b[0m     \u001b[32m1.12841\u001b[0m    0.34739      0.94039  3.73s\n",
      "      4     \u001b[36m0.18927\u001b[0m     \u001b[32m1.01890\u001b[0m    0.18576      0.94313  3.60s\n",
      "      5     \u001b[36m0.10324\u001b[0m     \u001b[32m0.99925\u001b[0m    0.10332      0.94539  3.64s\n",
      "      6     \u001b[36m0.05084\u001b[0m     \u001b[32m0.95761\u001b[0m    0.05309      0.94694  3.70s\n",
      "      7     \u001b[36m0.02757\u001b[0m     0.96898    0.02845      0.94717  4.41s\n",
      "      8     \u001b[36m0.01281\u001b[0m     \u001b[32m0.94990\u001b[0m    0.01349      0.94884  4.11s\n",
      "      9     \u001b[36m0.00665\u001b[0m     \u001b[32m0.91979\u001b[0m    0.00723      0.94860  4.64s\n",
      "     10     \u001b[36m0.00263\u001b[0m     \u001b[32m0.90479\u001b[0m    0.00290      0.94932  3.95s\n",
      "     11     \u001b[36m0.00074\u001b[0m     0.90870    0.00081      0.94979  4.91s\n",
      "     12     \u001b[36m0.00023\u001b[0m     0.90584    0.00025      0.94991  3.70s\n",
      "     13     \u001b[36m0.00009\u001b[0m     0.90599    0.00010      0.95027  4.80s\n",
      "     14     \u001b[36m0.00006\u001b[0m     0.90519    0.00007      0.95027  3.44s\n",
      "     15     \u001b[36m0.00005\u001b[0m     0.90482    0.00006      0.95039  3.45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x7fef9296fe90>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x7fef9296fd50>,\n",
       "     check_input=True, custom_scores=None, hidden_num_units=1000,\n",
       "     input_shape=(None, 1, 28, 28),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('hidden', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=15, more_params={},\n",
       "     objective=<function objective at 0x7fef9296aaa0>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0x7fef92a27050>,\n",
       "     on_batch_finished=[],\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog instance at 0x7fef92925440>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x7fef92925488>],\n",
       "     output_nonlinearity=<function softmax at 0x7fef92b91140>,\n",
       "     output_num_units=10, regression=False, scores_train=[],\n",
       "     scores_valid=[],\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x7fef9296ff10>,\n",
       "     update=<function nesterov_momentum at 0x7fef92a27938>,\n",
       "     update_learning_rate=0.0001, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_net.fit(train, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Implementing the neural network above gives us 0.95 accuracy. Although a bit lesser than our random classifier, this is still quite good by itself. \n",
    "\n",
    "Next, we'll be implementing convolutional neural network.\n",
    "\n",
    "A convolutional neural network (CNN) refers to a type of neural network which uses the convolution operator (often the 2D convolution when it is used for image processing tasks) to extract features from the data. In image processing, filters, that are convoluted with images, are learned automatically to solve the task at hand, e.g. a classification task.\n",
    "\n",
    "In our case, we will be using two convolutional layers (filtering), and one pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 443428 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name     size\n",
      "---  -------  --------\n",
      "  0  input    1x28x28\n",
      "  1  conv1    7x26x26\n",
      "  2  pool1    7x13x13\n",
      "  3  conv2    12x12x12\n",
      "  4  pool2    12x6x6\n",
      "  5  hidden3  1000\n",
      "  6  output   10\n",
      "\n",
      "  epoch    trn loss    val loss    trn/val    valid acc  dur\n",
      "-------  ----------  ----------  ---------  -----------  ------\n",
      "      1     \u001b[36m1.51190\u001b[0m     \u001b[32m0.43328\u001b[0m    3.48943      0.87008  22.04s\n",
      "      2     \u001b[36m0.33894\u001b[0m     \u001b[32m0.29825\u001b[0m    1.13640      0.91101  26.34s\n",
      "      3     \u001b[36m0.24318\u001b[0m     \u001b[32m0.24164\u001b[0m    1.00636      0.92838  28.04s\n",
      "      4     \u001b[36m0.19615\u001b[0m     \u001b[32m0.21022\u001b[0m    0.93308      0.94027  25.62s\n",
      "      5     \u001b[36m0.16692\u001b[0m     \u001b[32m0.18978\u001b[0m    0.87956      0.94432  25.70s\n"
     ]
    }
   ],
   "source": [
    "def CNN(n_epochs):\n",
    "    net1 = NeuralNet(\n",
    "        layers=[\n",
    "            ('input', layers.InputLayer),\n",
    "            ('conv1', layers.Conv2DLayer),\n",
    "            ('pool1', layers.MaxPool2DLayer),\n",
    "            ('conv2', layers.Conv2DLayer),\n",
    "            ('pool2', layers.MaxPool2DLayer),\n",
    "            ('hidden3', layers.DenseLayer),\n",
    "            ('output', layers.DenseLayer),\n",
    "        ],\n",
    "        input_shape=( None , 1, 28, 28),\n",
    "        conv1_num_filters=7,\n",
    "        conv1_filter_size=(3, 3),\n",
    "        conv1_nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        pool1_pool_size=(2, 2),\n",
    "        conv2_num_filters=12,\n",
    "        conv2_filter_size=(2, 2),\n",
    "        conv2_nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        pool2_pool_size=(2, 2),\n",
    "        hidden3_num_units=1000,\n",
    "        output_num_units=10,\n",
    "        output_nonlinearity=lasagne.nonlinearities.softmax,\n",
    "        update_learning_rate=0.0001,\n",
    "        update_momentum=0.9,\n",
    "        max_epochs=n_epochs,\n",
    "        verbose=1,\n",
    "        )\n",
    "    return net1\n",
    "cnn = CNN(5).fit(train,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, as the epoch increases, the accuracy increases until it tapers off at the maximum value. While the accuracy in 5 epochs arguably is not as high as that of our random classifier, we should note that as the amount of training data that we have increases our accuracy for CNN does as well. The main problem that we have with it later on would be overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
